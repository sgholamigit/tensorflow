{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 001 Step: 000 Loss: 2.19226\n",
      "Epoch: 001 Step: 020 Loss: 0.424353\n",
      "Epoch: 001 Step: 040 Loss: 0.197229\n",
      "Epoch: 001 Step: 060 Loss: 0.194211\n",
      "Epoch: 001 Step: 080 Loss: 0.134999\n",
      "Epoch: 001 Step: 100 Loss: 0.109169\n",
      "Epoch: 001 Step: 120 Loss: 0.015079\n",
      "Epoch: 001 Step: 140 Loss: 0.0359329\n",
      "Epoch: 001 Step: 160 Loss: 0.114146\n",
      "Epoch: 001 Step: 180 Loss: 0.0755346\n",
      "Epoch: 001 Step: 200 Loss: 0.0426837\n",
      "Epoch: 001 Step: 220 Loss: 0.0349889\n",
      "Epoch: 001 Step: 240 Loss: 0.0655355\n",
      "Epoch: 001 Step: 260 Loss: 0.0726925\n",
      "Epoch: 001 Step: 280 Loss: 0.0186648\n",
      "Epoch: 001 Step: 300 Loss: 0.121424\n",
      "Epoch: 001 Step: 320 Loss: 0.0245299\n",
      "Epoch: 001 Step: 340 Loss: 0.0637904\n",
      "Epoch: 001 Step: 360 Loss: 0.0525383\n",
      "Epoch: 001 Step: 380 Loss: 0.0304173\n",
      "Epoch: 001 Step: 400 Loss: 0.026476\n",
      "Epoch: 001 Step: 420 Loss: 0.0185745\n",
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-476a75de036d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtestX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m#batch_xs, batch_ys = mnist_data.test.next_batch(batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "# --------------------------------------\n",
    "# High-Level API: Using TFLearn wrappers\n",
    "# --------------------------------------\n",
    "\n",
    "# Using MNIST Dataset\n",
    "import tflearn.datasets.mnist as mnist\n",
    "mnist_data = mnist.read_data_sets(one_hot=True)\n",
    "\n",
    "# User defined placeholders\n",
    "with tf.Graph().as_default():\n",
    "    # Placeholders for data and labels\n",
    "    X = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=(None, 10), dtype=tf.float32)\n",
    "\n",
    "    net = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "    # Using TFLearn wrappers for network building\n",
    "    net = tflearn.conv_2d(net, 32, 3, activation='relu')\n",
    "    net = tflearn.max_pool_2d(net, 2)\n",
    "    net = tflearn.local_response_normalization(net)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.conv_2d(net, 64, 3, activation='relu')\n",
    "    net = tflearn.max_pool_2d(net, 2)\n",
    "    net = tflearn.local_response_normalization(net)\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 128, activation='tanh')\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 256, activation='tanh')\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 10, activation='linear')\n",
    "    net = regression(net, optimizer='adam', learning_rate=0.01,\n",
    "                     loss='categorical_crossentropy', name='target')\n",
    "\n",
    "    # Defining other ops using Tensorflow\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        batch_size = 128\n",
    "        for epoch in range(1): # 2 epochs\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist_data.train.num_examples/batch_size)\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist_data.train.next_batch(batch_size)\n",
    "                sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "                cost = sess.run(loss, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "                avg_cost += cost/total_batch\n",
    "                if i % 20 == 0:\n",
    "                    print(\"Epoch:\", '%03d' % (epoch+1), \"Step:\", '%03d' % i,\n",
    "                          \"Loss:\", str(cost))\n",
    "        X, Y, testX, testY = mnist.load_data(one_hot=True)\n",
    "        testX = testX.reshape([-1, 28, 28, 1])\n",
    "        #batch_xs, batch_ys = mnist_data.test.next_batch(batch_size)\n",
    "        sess.run(optimizer, feed_dict={X: testX, Y: testY})\n",
    "        cost = sess.run(loss, feed_dict={X: testX, Y: testY})\n",
    "        print(cost)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.120868\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
