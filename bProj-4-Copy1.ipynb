{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (100000, 28, 28) (100000,)\n",
      "Test set (1000, 28, 28) (1000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'brData10ClassSimple.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (2560000, 28, 28, 1) (2560000, 256)\n",
      "Test set (25600, 28, 28, 1) (25600, 256)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 256\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 245.883667\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 50: 5.546196\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 100: 5.545593\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 150: 5.546764\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 200: 5.544579\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 250: 5.544536\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 300: 5.545156\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 350: 5.545358\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 400: 5.544584\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 450: 5.544004\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 500: 5.544646\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 550: 5.543982\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 600: 5.543113\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 650: 5.545492\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 700: 5.545101\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 750: 5.545870\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 800: 5.545059\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 850: 5.546134\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 900: 5.544798\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 950: 5.546425\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1000: 5.544135\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1050: 5.545790\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1100: 5.545625\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1150: 5.545195\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 1200: 5.546083\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1250: 5.545816\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1300: 5.545295\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 1350: 5.545665\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 1400: 5.546285\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1450: 5.544900\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 1500: 5.547060\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1550: 5.545780\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1600: 5.546230\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1650: 5.545084\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1700: 5.545769\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 1750: 5.544890\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1800: 5.544862\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 1850: 5.544310\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1900: 5.543627\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 1950: 5.544999\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2000: 5.546178\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2050: 5.547018\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2100: 5.546077\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2150: 5.543840\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2200: 5.542642\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2250: 5.545703\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 2300: 5.548093\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2350: 5.547559\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2400: 5.546315\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2450: 5.545220\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2500: 5.544408\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2550: 5.544844\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2600: 5.544096\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 2650: 5.546319\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2700: 5.543697\n",
      "Minibatch accuracy: 3.0%\n",
      "Minibatch loss at step 2750: 5.545881\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 2800: 5.543826\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 2850: 5.544638\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2900: 5.544931\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 2950: 5.545469\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 3000: 5.545790\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 3050: 5.545143\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 3100: 5.544105\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 3150: 5.544833\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 3200: 5.544313\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 3250: 5.546719\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 3300: 5.547917\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 3350: 5.544340\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 3400: 5.544487\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 3450: 5.546104\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 3500: 5.546751\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 3550: 5.544647\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 3600: 5.544777\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 3650: 5.543947\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 3700: 5.545480\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 3750: 5.545417\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 3800: 5.545366\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 3850: 5.544806\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 3900: 5.546614\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 3950: 5.545595\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4000: 5.545699\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4050: 5.546618\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4100: 5.544757\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 4150: 5.546057\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 4200: 5.543513\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4250: 5.545713\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 4300: 5.545954\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4350: 5.545911\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4400: 5.544601\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4450: 5.543468\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4500: 5.544007\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4550: 5.548715\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4600: 5.545579\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4650: 5.545556\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4700: 5.547018\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4750: 5.546195\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4800: 5.545959\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4850: 5.545577\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4900: 5.544139\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4950: 5.543108\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5000: 5.546008\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5050: 5.547156\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 5100: 5.543695\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5150: 5.545150\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 5200: 5.542505\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5250: 5.545372\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5300: 5.544248\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5350: 5.547242\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5400: 5.544686\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5450: 5.546480\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5500: 5.543724\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 5550: 5.546022\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 5600: 5.544786\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5650: 5.545317\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5700: 5.546072\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 5750: 5.545344\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 5800: 5.547128\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 5850: 5.544806\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5900: 5.546649\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5950: 5.544542\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 6000: 5.544205\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 6050: 5.544887\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6100: 5.547175\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 6150: 5.546659\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6200: 5.545504\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 6250: 5.545323\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6300: 5.543557\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 6350: 5.544552\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6400: 5.543162\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 6450: 5.545225\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6500: 5.545016\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6550: 5.543892\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6600: 5.545543\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6650: 5.544546\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6700: 5.548895\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6750: 5.546260\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6800: 5.545709\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6850: 5.545199\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 6900: 5.544493\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 6950: 5.546119\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7000: 5.543813\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 7050: 5.542543\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7100: 5.545964\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7150: 5.545812\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7200: 5.542674\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 7250: 5.545709\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7300: 5.550126\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7350: 5.545912\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7400: 5.543407\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7450: 5.547594\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7500: 5.545842\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7550: 5.542749\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 7600: 5.544504\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 7650: 5.544597\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7700: 5.544560\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 7750: 5.544615\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 7800: 5.545482\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7850: 5.545993\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7900: 5.544294\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7950: 5.544435\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8000: 5.543765\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 8050: 5.546544\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8100: 5.545403\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8150: 5.546571\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 8200: 5.545175\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8250: 5.543730\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8300: 5.544957\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8350: 5.546339\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8400: 5.544621\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8450: 5.543947\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 8500: 5.543924\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 8550: 5.544475\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8600: 5.545322\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8650: 5.546751\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 8700: 5.545159\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8750: 5.545216\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8800: 5.545724\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8850: 5.545260\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8900: 5.544931\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8950: 5.545964\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 9000: 5.545285\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 9050: 5.544776\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 9100: 5.544581\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 9150: 5.544310\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 9200: 5.544364\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 9250: 5.544874\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 9300: 5.543417\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 9350: 5.544222\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 9400: 5.545411\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 9450: 5.545661\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 9500: 5.545547\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 9550: 5.549152\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 9600: 5.544391\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 9650: 5.544405\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 9700: 5.546466\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 9750: 5.545192\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 9800: 5.546152\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 9850: 5.541862\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 9900: 5.545519\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 9950: 5.545251\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 10000: 5.543499\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 10050: 5.543664\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 10100: 5.544935\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 10150: 5.547305\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 10200: 5.542268\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 10250: 5.546151\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 10300: 5.546376\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 10350: 5.545198\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 10400: 5.545119\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 10450: 5.545067\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 10500: 5.544877\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 10550: 5.545226\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 10600: 5.545347\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 10650: 5.546873\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 10700: 5.545718\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 10750: 5.543697\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 10800: 5.544292\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 10850: 5.542716\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 10900: 5.544096\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 10950: 5.543819\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 11000: 5.548965\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11050: 5.547733\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 11100: 5.545654\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 11150: 5.547559\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11200: 5.544418\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11250: 5.544628\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11300: 5.545872\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11350: 5.547584\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11400: 5.544434\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 11450: 5.544312\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11500: 5.546041\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11550: 5.545856\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11600: 5.545076\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11650: 5.545734\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11700: 5.544979\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 11750: 5.543678\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11800: 5.545458\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11850: 5.543911\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 11900: 5.546609\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 11950: 5.545774\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12000: 5.546371\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12050: 5.545959\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12100: 5.544440\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 12150: 5.545381\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12200: 5.541474\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12250: 5.544867\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12300: 5.547265\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12350: 5.543641\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 12400: 5.544803\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12450: 5.545005\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12500: 5.546459\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 12550: 5.546201\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 12600: 5.545087\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12650: 5.544983\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12700: 5.545943\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12750: 5.544464\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12800: 5.544897\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12850: 5.546103\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12900: 5.543883\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12950: 5.546230\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 13000: 5.543359\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13050: 5.545624\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13100: 5.546586\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13150: 5.544915\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13200: 5.545037\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 13250: 5.546411\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13300: 5.544144\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13350: 5.545269\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13400: 5.546352\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13450: 5.546733\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 13500: 5.544083\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 13550: 5.545962\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13600: 5.548014\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13650: 5.544150\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13700: 5.542905\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13750: 5.546419\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13800: 5.543292\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 13850: 5.545744\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 13900: 5.544672\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13950: 5.546488\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14000: 5.545910\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14050: 5.545803\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 14100: 5.546928\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14150: 5.546693\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14200: 5.546975\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14250: 5.544863\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14300: 5.548469\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14350: 5.542782\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 14400: 5.545286\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 14450: 5.545761\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 14500: 5.546781\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14550: 5.545663\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14600: 5.544376\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14650: 5.544076\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14700: 5.545024\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 14750: 5.546150\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14800: 5.545458\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14850: 5.544558\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14900: 5.545244\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 14950: 5.545392\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 15000: 5.544809\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15050: 5.546021\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15100: 5.544631\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 15150: 5.546204\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15200: 5.545281\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15250: 5.545565\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15300: 5.544813\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15350: 5.547411\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15400: 5.547529\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15450: 5.545578\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15500: 5.544767\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15550: 5.545550\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15600: 5.546707\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 15650: 5.544268\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15700: 5.546782\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15750: 5.544235\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15800: 5.544708\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15850: 5.544460\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15900: 5.544671\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15950: 5.543406\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16000: 5.548355\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16050: 5.544891\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16100: 5.546412\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16150: 5.544964\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 16200: 5.543880\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16250: 5.542727\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16300: 5.545842\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 16350: 5.545580\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16400: 5.545640\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16450: 5.543657\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16500: 5.542831\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16550: 5.546216\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16600: 5.546535\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 16650: 5.546491\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16700: 5.544242\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 16750: 5.545530\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16800: 5.545339\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16850: 5.545393\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16900: 5.548021\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16950: 5.546076\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 17000: 5.545858\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 17050: 5.547197\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17100: 5.544019\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17150: 5.544036\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17200: 5.545189\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17250: 5.545117\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17300: 5.546805\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 17350: 5.544782\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17400: 5.546999\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17450: 5.543313\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17500: 5.544659\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 17550: 5.546029\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 17600: 5.545953\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17650: 5.544485\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 17700: 5.544682\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17750: 5.545090\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17800: 5.545043\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17850: 5.544933\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17900: 5.544634\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 17950: 5.545381\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18000: 5.542923\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 18050: 5.545805\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18100: 5.541983\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 18150: 5.546078\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18200: 5.546559\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18250: 5.543632\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 18300: 5.544907\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 18350: 5.542853\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18400: 5.544701\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 18450: 5.542711\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18500: 5.544116\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18550: 5.545574\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18600: 5.546225\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18650: 5.544662\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18700: 5.543994\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18750: 5.544410\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18800: 5.545009\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18850: 5.549027\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18900: 5.544883\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 18950: 5.546011\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19000: 5.544461\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19050: 5.547044\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19100: 5.544188\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19150: 5.543891\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 19200: 5.543900\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 19250: 5.545410\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 19300: 5.545231\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19350: 5.545432\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19400: 5.545404\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19450: 5.545711\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19500: 5.544754\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19550: 5.543658\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19600: 5.545428\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19650: 5.545345\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 19700: 5.545761\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19750: 5.545641\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19800: 5.545062\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19850: 5.544022\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19900: 5.544996\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19950: 5.543443\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 20000: 5.543808\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20050: 5.545157\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20100: 5.547441\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 20150: 5.544464\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20200: 5.544475\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20250: 5.545406\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20300: 5.544901\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20350: 5.546135\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20400: 5.544505\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 20450: 5.543004\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20500: 5.543021\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 20550: 5.545295\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20600: 5.547324\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20650: 5.545352\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 20700: 5.543470\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 20750: 5.544880\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20800: 5.545456\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 20850: 5.546170\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20900: 5.545809\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20950: 5.543139\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21000: 5.544449\n",
      "Minibatch accuracy: 3.0%\n",
      "Minibatch loss at step 21050: 5.546204\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21100: 5.544876\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21150: 5.545409\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21200: 5.546737\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21250: 5.545983\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21300: 5.544083\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 21350: 5.543998\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21400: 5.543512\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 21450: 5.548497\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21500: 5.548061\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 21550: 5.544175\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 21600: 5.544048\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21650: 5.543673\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 21700: 5.546704\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21750: 5.546261\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21800: 5.547300\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21850: 5.542895\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21900: 5.545454\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21950: 5.543561\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22000: 5.544016\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 22050: 5.543527\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 22100: 5.545660\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22150: 5.544797\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22200: 5.545371\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22250: 5.543120\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22300: 5.545128\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22350: 5.544241\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22400: 5.546023\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22450: 5.546180\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 22500: 5.547900\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22550: 5.545479\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22600: 5.544131\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 22650: 5.547449\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 22700: 5.543385\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22750: 5.544895\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22800: 5.546794\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22850: 5.543138\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22900: 5.547246\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22950: 5.545452\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23000: 5.544797\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23050: 5.543898\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 23100: 5.546726\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23150: 5.544734\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23200: 5.543447\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23250: 5.545586\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23300: 5.545669\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 23350: 5.545700\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23400: 5.546463\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23450: 5.546013\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23500: 5.546176\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 23550: 5.548349\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23600: 5.543895\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 23650: 5.544468\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 23700: 5.547113\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23750: 5.547898\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23800: 5.542982\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 23850: 5.544536\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23900: 5.546414\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23950: 5.547086\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 24000: 5.546890\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 24050: 5.545546\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 24100: 5.548142\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 24150: 5.546162\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 24200: 5.545495\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 24250: 5.545355\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 24300: 5.543837\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 24350: 5.544939\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 24400: 5.543727\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 24450: 5.547389\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 24500: 5.545824\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 24550: 5.546164\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 24600: 5.546141\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 24650: 5.544817\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 24700: 5.544748\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 24750: 5.545236\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 24800: 5.544989\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 24850: 5.546875\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 24900: 5.545339\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 24950: 5.547366\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 25000: 5.542433\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 25050: 5.542539\n",
      "Minibatch accuracy: 2.0%\n",
      "Minibatch loss at step 25100: 5.544185\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 25150: 5.545946\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 25200: 5.546339\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 25250: 5.543392\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 25300: 5.542759\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 25350: 5.543957\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 25400: 5.546785\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 25450: 5.545898\n",
      "Minibatch accuracy: 1.0%\n",
      "Minibatch loss at step 25500: 5.547860\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 25550: 5.543718\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 25600: 5.542952\n",
      "Minibatch accuracy: 0.0%\n",
      "Test accuracy: 0.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 25601\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2242401\n",
      "153\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6H8BXs5CO+4lH9oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f22d9f83790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 2242401 is out of bounds for axis 0 with size 25600",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-21b33818b177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 2242401 is out of bounds for axis 0 with size 25600"
     ]
    }
   ],
   "source": [
    "#test the data\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "rand_image = random.randint(0, 2560000)\n",
    "print (rand_image)\n",
    "plt.imshow(train_dataset[rand_image], cmap='gray', interpolation='nearest', vmin=0, vmax=255)\n",
    "print (train_labels[rand_image])\n",
    "plt.show()\n",
    "np.mean(test_dataset[rand_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
