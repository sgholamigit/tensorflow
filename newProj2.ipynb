{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filename = 'trainsetpngResampled10000.tar.gz'\n",
    "test_filename = 'trainsetpngResampled100.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extracting the datasets\n",
    "num_classes = 256\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# have a look at data\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "def disp_samples(data_folders, sample_size):\n",
    "  for folder in data_folders:\n",
    "    print(folder)\n",
    "    image_files = os.listdir(folder)\n",
    "    image_sample = random.sample(image_files, sample_size)\n",
    "    for image in image_sample:\n",
    "      image_file = os.path.join(folder, image)\n",
    "      i = Image(filename=image_file)\n",
    "      display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disp_samples(train_folders, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disp_samples(test_folders, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merging the data\n",
    "\n",
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "num_of_classes = 256\n",
    "\n",
    "        \n",
    "def Merge_folders(data_folders, size_per_class):\n",
    "  dataset_names = []\n",
    "  start_t = 0\n",
    "  end_t = size_per_class\n",
    "  required_size = size_per_class * num_of_classes\n",
    "  trainDataset = np.ndarray((required_size, image_size, image_size), dtype=np.uint8)\n",
    "  labelsDataset = np.ndarray(required_size, dtype=np.int32)\n",
    "  for folder in data_folders:\n",
    "    dataset_names.append(folder)\n",
    "\n",
    "    print('Merging %s.' % folder)\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.uint8)\n",
    "    image_index = 0\n",
    "    for image in os.listdir(folder):\n",
    "        if image_index < size_per_class:\n",
    "            image_file = os.path.join(folder, image)\n",
    "            image_data = (ndimage.imread(image_file).astype(int))\n",
    "            dataset[image_index, :, :] = image_data\n",
    "            image_index += 1\n",
    "    num_images = image_index\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    trainDataset[start_t:end_t, :, :] = dataset\n",
    "    labelsDataset[start_t:end_t] = folder.split(\"/\")[1]\n",
    "    start_t += size_per_class\n",
    "    end_t += size_per_class\n",
    "\n",
    "  \n",
    "  return dataset_names, trainDataset, labelsDataset\n",
    "\n",
    "train_datasets, trainDataset, labelsTrainDataset = Merge_folders(train_folders, 10000)\n",
    "test_datasets, testDataset, labelsTestDataset = Merge_folders(test_folders, 100)\n",
    "\n",
    "print(trainDataset)\n",
    "print(labelsTrainDataset)\n",
    "\n",
    "print(testDataset)\n",
    "print(labelsTestDataset)\n",
    "\n",
    "print('trainDataset.shape' , trainDataset.shape)\n",
    "print('labelsTrainDataset.shape' , labelsTrainDataset.shape)\n",
    "\n",
    "print('testDataset.shape' , testDataset.shape)\n",
    "print('labelsTestDataset.shape' , labelsTestDataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test the data\n",
    "rand_image = random.randint(0, 2560000)\n",
    "print (rand_image)\n",
    "plt.imshow(trainDataset[rand_image], cmap='gray', interpolation='nearest', vmin=0, vmax=255)\n",
    "print (labelsTrainDataset[rand_image])\n",
    "plt.show()\n",
    "np.mean(trainDataset[rand_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomize the data\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(trainDataset, labelsTrainDataset)\n",
    "test_dataset, test_labels = randomize(testDataset, labelsTestDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test the data\n",
    "rand_image = random.randint(0, 2560000)\n",
    "print (rand_image)\n",
    "plt.imshow(train_dataset[rand_image], cmap='gray', interpolation='nearest', vmin=0, vmax=255)\n",
    "print (train_labels[rand_image])\n",
    "plt.show()\n",
    "np.mean(train_dataset[rand_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test the data\n",
    "rand_image = random.randint(0, 25600)\n",
    "print (rand_image)\n",
    "plt.imshow(test_dataset[rand_image], cmap='gray', interpolation='nearest', vmin=0, vmax=255)\n",
    "print (test_labels[rand_image])\n",
    "plt.show()\n",
    "np.mean(test_dataset[rand_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the data\n",
    "pickle_file = 'brightnessData.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#estimate the duplicates\n",
    "\n",
    "all_data = pickle.load(open('brightnessData.pickle', 'rb'))\n",
    "\n",
    "def count_duplicates(dataset1, dataset2):\n",
    "    hashes = [hashlib.sha1(x).hexdigest() for x in dataset1]\n",
    "    dup_indices = []\n",
    "    for i in range(0, len(dataset2)):\n",
    "        if hashlib.sha1(dataset2[i]).hexdigest() in hashes:\n",
    "            dup_indices.append(i)\n",
    "    return len(dup_indices)\n",
    "\n",
    "\n",
    "print(count_duplicates(all_data['test_dataset'], all_data['train_dataset']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test a regression model\n",
    "\n",
    "train_dataset = all_data['train_dataset']\n",
    "train_labels = all_data['train_labels']\n",
    "test_dataset = all_data['test_dataset']\n",
    "test_labels = all_data['test_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_score(train_dataset, train_labels, test_dataset, test_labels):\n",
    "    model = LogisticRegression()\n",
    "    train_flatten_dataset = np.array([x.flatten() for x in train_dataset])\n",
    "    test_flatten_dataset = np.array([x.flatten() for x in test_dataset])\n",
    "    model.fit(train_flatten_dataset, train_labels)\n",
    "\n",
    "    return model.score([x.flatten() for x in test_dataset], test_labels)\n",
    "\n",
    "print(\"100 trainsamples score: \" + str(get_score(train_dataset[:100], train_labels[:100], test_dataset, test_labels)))\n",
    "print(\"1000 trainsamples score: \" + str(get_score(train_dataset[:1000], train_labels[:1000], test_dataset, test_labels)))\n",
    "print(\"5000 trainsamples score: \" + str(get_score(train_dataset[:5000], train_labels[:5000], test_dataset, test_labels)))\n",
    "print(\"10000 trainsamples score: \" + str(get_score(train_dataset[:10000], train_labels[:10000], test_dataset, test_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
