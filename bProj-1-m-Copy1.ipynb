{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle file exists!\n",
      "Training set (260000, 28, 28) (260000,)\n",
      "Test set (2600, 28, 28) (2600,)\n",
      "Randomizing is done\n",
      "Training set (260000, 28, 28) (260000,)\n",
      "Test set (2600, 28, 28) (2600,)\n",
      "Reformatted training set (260000, 28, 28, 1) (260000, 26)\n",
      "Reformatted test set (2600, 28, 28, 1) (2600, 26)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "\n",
    "\n",
    "#pickle_file = 'train7d10class10000.pickle'\n",
    "#pickle_file = 'brSimpleclass30.pickle'\n",
    "pickle_file = 'train10step7Domain10000.pickle'\n",
    "\n",
    "# Basic model parameters as external flags.\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "# Randomizing again the dataset\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, FLAGS.image_size, FLAGS.image_size, FLAGS.num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(FLAGS.num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  try:\n",
    "    pickle_file\n",
    "    print (\"pickle file exists!\")\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "      save = pickle.load(f)\n",
    "      train_dataset = save['train_dataset']\n",
    "      train_labels = save['train_labels']\n",
    "      test_dataset = save['test_dataset']\n",
    "      test_labels = save['test_labels']\n",
    "      del save  # hint to help gc free up memory\n",
    "      print('Training set', train_dataset.shape, train_labels.shape)\n",
    "      print('Test set', test_dataset.shape, test_labels.shape)\n",
    "  except NameError:\n",
    "    print (\"pickle file does not exist!\")\n",
    "\n",
    "  # randomize the data\n",
    "  train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "  test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "  print('Randomizing is done')\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "  # Reformat the data: image data as a cube (width by height by number \n",
    "  # of channels) and labels as float 1-hot encodings.\n",
    "  train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "  test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "  print('Reformatted training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Reformatted test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "  def accuracy(predictions, labels): \n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "  def accuracy2(predictions, labels):\n",
    "    print ('shape = ', np.argmax(predictions, 1).shape)\n",
    "    sh = np.argmax(predictions, 1).shape\n",
    "    predArg = np.argmax(predictions, 1)\n",
    "    labArg = np.argmax(labels, 1)  \n",
    "    count = 0\n",
    "    pred = []\n",
    "    lab = []\n",
    "    print ('sh[0]', sh[0])\n",
    "    \n",
    "    for i in range(sh[0]):\n",
    "      #print (i)\n",
    "      if predArg[i] != labArg[i]:\n",
    "        count = count + 1\n",
    "        #print ('pred = ', predArg[i])\n",
    "        #print ('lab = ', labArg[i]) \n",
    "        pred.append(predArg[i])\n",
    "        lab.append(labArg[i])\n",
    "        #print ('prediction =', predictions[i,:])\n",
    "        #print ('labels =', labels[i,:])  \n",
    "    print ('count =', count)  \n",
    "    print ('pred =', pred)\n",
    "    print ('lab =', lab)\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "  def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "  def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "  def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "  def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "\t                  strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "  def drop_out(x, keep_prob):\n",
    "    return tf.nn.dropout(x, keep_prob)\n",
    "\n",
    "  graph = tf.Graph()\n",
    "\n",
    "  with graph.as_default():\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "      tf.float32, shape=(FLAGS.batch_size, FLAGS.image_size, FLAGS.image_size, FLAGS.num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, FLAGS.num_labels))\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    W_conv1 = weight_variable([5, 5, 1, 20])\n",
    "    b_conv1 = bias_variable([20])\n",
    "\n",
    "    W_conv2 = weight_variable([5, 5, 20, 50])\n",
    "    b_conv2 = bias_variable([50])\n",
    "\n",
    "    W_fc1 = weight_variable([7 * 7 * 50, 500])\n",
    "    b_fc1 = bias_variable([500])\n",
    "\n",
    "    W_fc2 = weight_variable([500, FLAGS.num_labels])\n",
    "    b_fc2 = bias_variable([FLAGS.num_labels])\n",
    "\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "      h_conv1 = tf.nn.relu(conv2d(data, W_conv1) + b_conv1)\n",
    "      h_pool1 = max_pool_2x2(h_conv1)\n",
    "      h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "      h_pool2 = max_pool_2x2(h_conv2)\n",
    "      shape = h_pool2.get_shape().as_list()\n",
    "      reshape = tf.reshape(h_pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "      h_fc1 = tf.nn.tanh(tf.matmul(reshape, W_fc1) + b_fc1)\n",
    "      return tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "   \n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "\n",
    "    step_plot = []\n",
    "    loss_plot = []\n",
    "    accuracy_plot = []\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "\n",
    "    print('Initialized')\n",
    "    for step in range(FLAGS.max_steps):\n",
    "      offset = (step * FLAGS.batch_size) % (train_labels.shape[0] - FLAGS.batch_size)\n",
    "      batch_data = train_dataset[offset:(offset + FLAGS.batch_size), :, :, :]\n",
    "      batch_labels = train_labels[offset:(offset + FLAGS.batch_size), :]\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "      _, l, predictions = sess.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      if (step % 50 == 0):\n",
    "        print('Minibatch loss at step %d: %f' % (step, l))\n",
    "        acc = accuracy(predictions, batch_labels)\n",
    "        print('Minibatch accuracy: %.1f%%' % acc)\n",
    "        step_plot.append(step)\n",
    "        loss_plot.append(l)\n",
    "        accuracy_plot.append(acc)\n",
    "    print('Test accuracy: %.1f%%' % accuracy2(test_prediction.eval(session=sess), test_labels))\n",
    "    print (test_prediction.eval(session=sess).shape)\n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(step_plot, loss_plot, 'rs--')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss plot')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(step_plot, accuracy_plot, '--bs')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy plot')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #print(accuracy_plot)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      type=float,\n",
    "      default=0.005,\n",
    "      help='Initial learning rate.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--max_steps',\n",
    "      type=int,\n",
    "      default=1001,\n",
    "      help='Number of steps to run trainer.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--batch_size',\n",
    "      type=int,\n",
    "      default=100,\n",
    "      help='Batch size.  Must divide evenly into the dataset sizes.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--number_classses',\n",
    "      type=int,\n",
    "      default=256,\n",
    "      help='Number of classes in train and test data sets.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--image_size',\n",
    "      type=int,\n",
    "      default=28,\n",
    "      help='Size of the image.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--pixel_depth',\n",
    "      type=float,\n",
    "      default=255.0,\n",
    "      help='Number of levels per pixel.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--num_channels',\n",
    "      type=int,\n",
    "      default=1,\n",
    "      help='1 for grayscale'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--num_images_Train',\n",
    "      type=int,\n",
    "      default=10000,\n",
    "      help='Number of images per label in train floder'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--num_images_Test',\n",
    "      type=int,\n",
    "      default=100,\n",
    "      help='Number of images per label in test floder'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--num_labels',\n",
    "      type=int,\n",
    "      default=26,\n",
    "      help='Number of labels'\n",
    "  )\n",
    "\n",
    " \n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filename = 'train7d10class10000'\n",
    "test_filename = 'test7d10class10000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train7d10class10000 already present - Skipping extraction of train7d10class10000.\n",
      "['train7d10class10000/0', 'train7d10class10000/105', 'train7d10class10000/135', 'train7d10class10000/15', 'train7d10class10000/165', 'train7d10class10000/195', 'train7d10class10000/225', 'train7d10class10000/255', 'train7d10class10000/45', 'train7d10class10000/75']\n",
      "test7d10class10000 already present - Skipping extraction of test7d10class10000.\n",
      "['test7d10class10000/0', 'test7d10class10000/105', 'test7d10class10000/135', 'test7d10class10000/15', 'test7d10class10000/165', 'test7d10class10000/195', 'test7d10class10000/225', 'test7d10class10000/255', 'test7d10class10000/45', 'test7d10class10000/75']\n"
     ]
    }
   ],
   "source": [
    "# extracting the datasets\n",
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# have a look at data\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "def disp_samples(data_folders, sample_size):\n",
    "  for folder in data_folders:\n",
    "    print(folder)\n",
    "    image_files = os.listdir(folder)\n",
    "    image_sample = random.sample(image_files, sample_size)\n",
    "    for image in image_sample:\n",
    "      image_file = os.path.join(folder, image)\n",
    "      i = Image(filename=image_file)\n",
    "      display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train7d10class10000/0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAAB\nPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train7d10class10000/105\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAARklEQVR4nGMUYWBgYGBgmAyhrkIo\nDwjFxIAHDDpJlk8Q+h6EiodQmgPnIPIlGbdD6E0QaguE4hk4B1HglUxsoooD5yDyJQEpUAcHz73b\nVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train7d10class10000/135\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAS0lEQVR4nGN8x8DAwMDA8B5CPYJQ\nOyAUEwMeMOgkWbog9DcI5Q6hBAbOQRR4hR9CX4dQUI95DZyDyJdk1IfQnRDqPITaNXAOIl8SAN+q\nCYRTbtjOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train7d10class10000/15\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAARklEQVR4nGPkZGBgYGBgYIZQfyHU\nDwjFxIAHDDpJFkYI/RNCZUKo+QPnIPIlGXkh9D8I9WfAHUSBV6Cy3BAK1UeDzrV4JQHYUAhJ9uwu\nhwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train7d10class10000/165\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAASElEQVR4nGPMY2BgYGBg4IZQ+yHU\nPAjFxIAHDDpJxmcQ+iKE4oFQlwfOQRR4JQhCB6OImg2cgyjwyjEI/Q1CvYJQCgPnIPIlAYvECEIv\n7fk8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train7d10class10000/195\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAR0lEQVR4nGNcxcDAwMDAIAuhOCGU\nBIRiYsADBp0k43cU7gMI9WPgHESBV0wh9EQIVQ6hagbOQRR45Q2E5odyIdTigXMQ+ZIAnWQHRL8d\nWBIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train7d10class10000/225\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAS0lEQVR4nGN8ycDAwMDA8B9CBUKo\nYxCKiQEPGHSSjJ8hNAuE2gehTAfOQRR45SeEfgWheCAU68A5iHxJFkYIzQWhOCDUq4FzEPmSAPz4\nCBJhar+WAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train7d10class10000/255\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAFklEQVR4nGP8z4AbMOGRG5UclRxh\nkgCD/gE3CChK/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train7d10class10000/45\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAASElEQVR4nGMUYGBgYGBg+AKhpCDU\nYwjFxIAHDDpJFlkU7i8ItXHgHESBV6A0NB44INSjgXMQ+ZKMuhD6CYSSgFAfB85B5EsCAA6YB+s2\nnOe/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train7d10class10000/75\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAASElEQVR4nGP0YWBgYGBgYINQWhBq\nMoRiYsADBp0kyxcI/QJCSUMo0YFzEAVe8YPQGyDU2QF3EPmSjHoQGpqyTCDUsYFzEPmSAD9hBnMW\niFivAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp_samples(train_folders, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test7d10class10000/0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAEElEQVR4nGNgGAWjYBQQAwADLAAB\nPwpG8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test7d10class10000/105\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAS0lEQVR4nGPUYWBgYGBgiIZQPyHU\nFQjFxIAHDDpJxgsQehKEEoBQ8gPnIAq84g+hhSDUfwh1fuAcRIFXTkDoCgilDKGOD5yDyJcEADxN\nCHPfoMklAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test7d10class10000/135\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAS0lEQVR4nGOcyMDAwMDAcBNCfYBQ\nihCKiQEPGHSSLDcgtB2E0oVQ3wbOQeRLMp6E0NYQahKEujZwDqLAK24QOhBCJUGokIFzEPmSAAZa\nCKUWrEdJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test7d10class10000/15\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAR0lEQVR4nGPkY2BgYGBg+AehfCHU\nVgjFxIAHDDpJRh4I/QdCsUCo3wPnIAq8woGiiBGFGnSuxSvJwgyhobEC5f0YOAeRLwkA/j4GUxls\nYmgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test7d10class10000/165\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAS0lEQVR4nGN8wMDAwMDAMB1CJUKo\n9RCKiQEPGHSSjFsh9H8I9R5CeQ+cg8iXZOGD0FYQyg1Fy6BzLf5Y2QGhP0KomxBq8sA5iHxJAFBF\nCVIm70IQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test7d10class10000/195\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAASUlEQVR4nGP8z8DAwMDA8ANCnYFQ\nChCKiQEPGHSSLPch9H8UUcWBcxD5koyJELoBQj2CUDsHzkEUeOUwhDaAUO8hlPjAOYh8SQBxVQmP\nNmoyMAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test7d10class10000/225\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAR0lEQVR4nGN8yMDAwMDAwAqhBCHU\nBwjFxIAHDDpJlp0Q2gNCvYdQfwbOQeRLMj5E4QpAqA8D5yAKYoULQn+DUF8H3EHkSwIAfCoI5BSf\ns9UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test7d10class10000/255\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAFklEQVR4nGP8z4AbMOGRG5UclRxh\nkgCD/gE3CChK/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test7d10class10000/45\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAASElEQVR4nGNkY2BgYGBg+AehWCAU\nI4RiYsADBp0k4z0IbQWhmCHU14FzEAVeUYHQohDqEYTqHTgHUe4VaJK6DaG0B85B5EsCAMr4BiG/\ntaNqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test7d10class10000/75\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAS0lEQVR4nGM0YmBgYGBgyINQqRCK\nBUIxMeABg06S8QCEToZQDyGUxsA5iAKvqEDovxBKHkJdHzgHkS/JwgyhyyCUIoR6NXAOIl8SAP08\nBwiKlTdkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp_samples(test_folders, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging train7d10class10000/0.\n",
      "Merging train7d10class10000/105.\n",
      "Merging train7d10class10000/135.\n",
      "Merging train7d10class10000/15.\n",
      "Merging train7d10class10000/165.\n",
      "Merging train7d10class10000/195.\n",
      "Merging train7d10class10000/225.\n",
      "Merging train7d10class10000/255.\n",
      "Merging train7d10class10000/45.\n",
      "Merging train7d10class10000/75.\n",
      "Merging test7d10class10000/0.\n",
      "Merging test7d10class10000/105.\n",
      "Merging test7d10class10000/135.\n",
      "Merging test7d10class10000/15.\n",
      "Merging test7d10class10000/165.\n",
      "Merging test7d10class10000/195.\n",
      "Merging test7d10class10000/225.\n",
      "Merging test7d10class10000/255.\n",
      "Merging test7d10class10000/45.\n",
      "Merging test7d10class10000/75.\n",
      "[[[  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  ..., \n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]]\n",
      "\n",
      " [[  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  ..., \n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]]\n",
      "\n",
      " [[  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  ..., \n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]]\n",
      "\n",
      " ..., \n",
      " [[ 37  37  37 ...,  69  69  69]\n",
      "  [ 37  37  37 ...,  69  69  69]\n",
      "  [ 37  37  37 ...,  69  69  69]\n",
      "  ..., \n",
      "  [ 75  75  75 ..., 189 189 189]\n",
      "  [ 75  75  75 ..., 189 189 189]\n",
      "  [ 75  75  75 ..., 189 189 189]]\n",
      "\n",
      " [[ 75  75  75 ..., 198 198 198]\n",
      "  [ 75  75  75 ..., 198 198 198]\n",
      "  [ 75  75  75 ..., 198 198 198]\n",
      "  ..., \n",
      "  [191 191 191 ...,  69  69  69]\n",
      "  [191 191 191 ...,  69  69  69]\n",
      "  [191 191 191 ...,  69  69  69]]\n",
      "\n",
      " [[ 62  62  62 ..., 255 255 255]\n",
      "  [ 62  62  62 ..., 255 255 255]\n",
      "  [ 62  62  62 ..., 255 255 255]\n",
      "  ..., \n",
      "  [ 27  27  27 ...,   7   7   7]\n",
      "  [ 27  27  27 ...,   7   7   7]\n",
      "  [ 27  27  27 ...,   7   7   7]]]\n",
      "[ 0  0  0 ..., 75 75 75]\n",
      "[[[  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  ..., \n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]]\n",
      "\n",
      " [[  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  ..., \n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]]\n",
      "\n",
      " [[  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  ..., \n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]\n",
      "  [  0   0   0 ...,   0   0   0]]\n",
      "\n",
      " ..., \n",
      " [[124 124 124 ...,   1   1   1]\n",
      "  [124 124 124 ...,   1   1   1]\n",
      "  [124 124 124 ...,   1   1   1]\n",
      "  ..., \n",
      "  [ 23  23  23 ...,  54  54  54]\n",
      "  [ 23  23  23 ...,  54  54  54]\n",
      "  [ 23  23  23 ...,  54  54  54]]\n",
      "\n",
      " [[ 29  29  29 ...,  64  64  64]\n",
      "  [ 29  29  29 ...,  64  64  64]\n",
      "  [ 29  29  29 ...,  64  64  64]\n",
      "  ..., \n",
      "  [ 81  81  81 ...,  52  52  52]\n",
      "  [ 81  81  81 ...,  52  52  52]\n",
      "  [ 81  81  81 ...,  52  52  52]]\n",
      "\n",
      " [[ 75  75  75 ...,  31  31  31]\n",
      "  [ 75  75  75 ...,  31  31  31]\n",
      "  [ 75  75  75 ...,  31  31  31]\n",
      "  ..., \n",
      "  [ 29  29  29 ...,   9   9   9]\n",
      "  [ 29  29  29 ...,   9   9   9]\n",
      "  [ 29  29  29 ...,   9   9   9]]]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0 105 105 105 105 105 105 105 105\n",
      " 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n",
      " 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n",
      " 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n",
      " 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n",
      " 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n",
      " 105 105 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135\n",
      " 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135\n",
      " 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135\n",
      " 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135\n",
      " 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135 135\n",
      " 135 135 135 135 135 135 135 135 135 135 135 135  15  15  15  15  15  15\n",
      "  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15\n",
      "  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15\n",
      "  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15\n",
      "  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15\n",
      "  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15\n",
      "  15  15  15  15 165 165 165 165 165 165 165 165 165 165 165 165 165 165\n",
      " 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165\n",
      " 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165\n",
      " 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165\n",
      " 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165\n",
      " 165 165 165 165 165 165 165 165 165 165 165 165 165 165 195 195 195 195\n",
      " 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195\n",
      " 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195\n",
      " 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195\n",
      " 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195\n",
      " 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195 195\n",
      " 195 195 195 195 195 195 225 225 225 225 225 225 225 225 225 225 225 225\n",
      " 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225\n",
      " 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225\n",
      " 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225\n",
      " 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225\n",
      " 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 225 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255  45  45  45  45  45  45  45  45  45  45\n",
      "  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45\n",
      "  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45\n",
      "  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45\n",
      "  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45\n",
      "  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45  45\n",
      "  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75\n",
      "  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75\n",
      "  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75\n",
      "  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75\n",
      "  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75  75\n",
      "  75  75  75  75  75  75  75  75  75  75]\n",
      "trainDataset.shape (100000, 28, 28)\n",
      "labelsTrainDataset.shape (100000,)\n",
      "testDataset.shape (1000, 28, 28)\n",
      "labelsTestDataset.shape (1000,)\n"
     ]
    }
   ],
   "source": [
    "#Merging the data\n",
    "\n",
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "num_of_classes = 10\n",
    "\n",
    "        \n",
    "def Merge_folders(data_folders, size_per_class):\n",
    "  dataset_names = []\n",
    "  start_t = 0\n",
    "  end_t = size_per_class\n",
    "  required_size = size_per_class * num_of_classes\n",
    "  trainDataset = np.ndarray((required_size, image_size, image_size), dtype=np.uint8)\n",
    "  labelsDataset = np.ndarray(required_size, dtype=np.int32)\n",
    "  for folder in data_folders:\n",
    "    dataset_names.append(folder)\n",
    "\n",
    "    print('Merging %s.' % folder)\n",
    "    image_files = os.listdir(folder)\n",
    "    np.random.shuffle(image_files)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.uint8)\n",
    "    image_index = 0\n",
    "    for image in image_files:\n",
    "        if image_index < size_per_class:\n",
    "            image_file = os.path.join(folder, image)\n",
    "            image_data = (ndimage.imread(image_file).astype(int))\n",
    "            dataset[image_index, :, :] = image_data\n",
    "            image_index += 1\n",
    "    num_images = image_index\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    trainDataset[start_t:end_t, :, :] = dataset\n",
    "    labelsDataset[start_t:end_t] = folder.split(\"/\")[1]\n",
    "    start_t += size_per_class\n",
    "    end_t += size_per_class\n",
    "\n",
    "  \n",
    "  return dataset_names, trainDataset, labelsDataset\n",
    "\n",
    "train_datasets, trainDataset, labelsTrainDataset = Merge_folders(train_folders, 10000)\n",
    "test_datasets, testDataset, labelsTestDataset = Merge_folders(test_folders, 100)\n",
    "\n",
    "print(trainDataset)\n",
    "print(labelsTrainDataset)\n",
    "\n",
    "print(testDataset)\n",
    "print(labelsTestDataset)\n",
    "\n",
    "print('trainDataset.shape' , trainDataset.shape)\n",
    "print('labelsTrainDataset.shape' , labelsTrainDataset.shape)\n",
    "\n",
    "print('testDataset.shape' , testDataset.shape)\n",
    "print('labelsTestDataset.shape' , labelsTestDataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12008\n",
      "105\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWJISGWFJSmSEJSmREZakREZYkhL9L3EkQ9NSNDlbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb1f948ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "105.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the data\n",
    "rand_image = random.randint(0, 20000)\n",
    "print (rand_image)\n",
    "plt.imshow(trainDataset[rand_image], cmap='gray', interpolation='nearest', vmin=0, vmax=255)\n",
    "print (labelsTrainDataset[rand_image])\n",
    "plt.show()\n",
    "np.mean(trainDataset[rand_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([255,   0,  75, ...,  75,   0, 195], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomize the data\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(trainDataset, labelsTrainDataset)\n",
    "test_dataset, test_labels = randomize(testDataset, labelsTestDataset)\n",
    "\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10864\n",
      "75\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb1f948c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "75.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the data\n",
    "rand_image = random.randint(0, 20000)\n",
    "print (rand_image)\n",
    "plt.imshow(train_dataset[rand_image], cmap='gray', interpolation='nearest', vmin=0, vmax=255)\n",
    "print (train_labels[rand_image])\n",
    "plt.show()\n",
    "np.mean(train_dataset[rand_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAD3hJREFUeJzt3X+opQWdx/H3ZxvDnBBJlxlYI7WpP/wjSVtdN61pR7D8\nw/YvwwJp+0PEXBZhSYLY2dU/IsNwKWaJhdWiNRBa13ZRpxLbEtcmFCMTV3THTMe5qwkK/ihzvvvH\ncy4crzN3PPee4/eeM+8XPOB5zjP3+T4+17fPfe7jMVWFJKnHH3UPIElHMiMsSY2MsCQ1MsKS1MgI\nS1IjIyxJjYywJDUywpLUyAhLUqNN3QMkOR44H3gceKV3GkmaiqOBk4DdVfXb1TacWYSTfB74W2Ar\n8Avgr6vq5wfZ9HzgX2c1hyQ1+gxw02obzOR2RJJPAdcBO4EPMkR4d5ITDrL547OYQZI2gMcPt8Gs\n7glfCXyzqr5dVQ8DlwEvAZ87yLbegpC0qA7bt6lHOMlRwBnAncvravioth8BZ097f5I0z2ZxJXwC\n8DZgacX6JYb7w5KkER9Rk6RGs4jws8BrwJYV67cA+2ewP0maW1OPcFW9CtwH7FhelySj1/dMe3+S\nNM9m9Zzw14Abk9wH7GF4WuIY4MYZ7U+S5tJMIlxVN4+eCb6a4TbEA8D5VfXMLPYnSfMq3f+jzySn\nM9y+kKRFc0ZV3b/aBj4dIUmNjLAkNTLCktTICEtSIyMsSY2MsCQ1MsKS1MgIS1IjIyxJjYywJDUy\nwpLUyAhLUiMjLEmNjLAkNTLCktTICEtSIyMsSY2MsCQ1MsKS1MgIS1IjIyxJjYywJDUywpLUyAhL\nUiMjLEmNjLAkNTLCktTICEtSIyMsSY2MsCQ1MsKS1MgIS1IjIyxJjYywJDUywpLUyAhLUiMjLEmN\njLAkNTLCktTICEtSIyMsSY2MsCQ1MsKS1MgIS1KjqUc4yc4kB1YsD017P5K0CDbN6Os+COwAMnr9\nhxntR5Lm2qwi/IeqemZGX1uSFsas7gm/L8lTSR5L8p0k757RfiRprs0iwvcCnwXOBy4DTgZ+kmTz\nDPYlSXNt6rcjqmr32MsHk+wBfg1cBNww7f1J0jyb+SNqVfU88Aiwbdb7kqR5M/MIJ3knQ4CfnvW+\nJGnezOI54a8m+UiS9yT5c+AW4FXgu9PelyTNu1k8onYicBNwPPAMcDfwZ1X12xnsS5Lm2ix+MXfx\ntL+mJC0qPztCkhoZYUlqZIQlqZERlqRGRliSGhlhSWpkhCWpkRGWpEZGWJIaGWFJamSEJamREZak\nRkZYkhoZYUlqZIQlqZERlqRGRliSGhlhSWpkhCWpkRGWpEZGWJIaGWFJamSEJamREZakRkZYkhoZ\nYUlqZIQlqZERlqRGRliSGhlhSWpkhCWpkRGWpEZGWJIaGWFJamSEJamREZakRkZYkhoZYUlqZIQl\nqZERlqRGRliSGhlhSWpkhCWpkRGWpEZGWJIaTRzhJOcm+X6Sp5IcSHLhQba5Osm+JC8l+WGSbdMZ\nV5IWy1quhDcDDwCXA7XyzSRXAVcAlwJnAi8Cu5O8fR1zStJC2jTpH6iqO4A7AJLkIJv8DXBNVf3n\naJtLgCXgL4Gb1z6qJC2eqd4TTnIysBW4c3ldVb0A/Aw4e5r7kqRFMO1fzG1luEWxtGL90ug9SdIY\nn46QpEbTjvB+IMCWFeu3jN6TJI2ZaoSrai9DbHcsr0tyLHAWcM809yVJi2DipyOSbAa2MVzxApyS\n5DTguar6DXA98KUkjwKPA9cATwK3TmViSVogE0cY+BBwF8Mv4Aq4brT+W8DnquraJMcA3wSOA34K\nfKKqfj+FeSVpoaTqDf+9xVs7QHI6cF/rEJI0G2dU1f2rbeDTEZLUyAhLUiMjLEmNjLAkNTLCktTI\nCEtSIyMsSY2MsCQ1MsKS1MgIS1IjIyxJjYywJDUywpLUyAhLUiMjLEmNjLAkNTLCktTICEtSIyMs\nSY2MsCQ1MsKS1MgIS1IjIyxJjYywJDUywpLUyAhLUiMjLEmNjLAkNTLCktTICEtSIyMsSY2MsCQ1\nMsKS1MgIS1IjIyxJjYywJDUywpLUyAhLUiMjLEmNjLAkNTLCktTICEtSIyMsSY2MsCQ1mjjCSc5N\n8v0kTyU5kOTCFe/fMFo/vtw2vZElaXGs5Up4M/AAcDlQh9jmdmALsHW0XLym6SRpwW2a9A9U1R3A\nHQBJcojNfldVz6xnMEk6EszqnvD2JEtJHk6yK8m7ZrQfSZprE18Jvwm3A98D9gLvBb4M3Jbk7Ko6\n1O0LSToiTT3CVXXz2MtfJfkl8BiwHbhr2vuTpHk280fUqmov8Cywbdb7kqR5M/MIJzkROB54etb7\nkqR5M/HtiCSbGa5ql5+MOCXJacBzo2Unwz3h/aPtvgI8AuyexsCStEjWck/4Qwz3dmu0XDda/y2G\nZ4c/AFwCHAfsY4jv31XVq+ueVpIWzFqeE/4vVr+N8fG1jyNJRxY/O0KSGhlhSWpkhCWpkRGWpEZG\nWJIaGWFJamSEJamREZakRkZYkhoZYUlqZIQlqZERlqRGRliSGhlhSWpkhCWpkRGWpEZGWJIaGWFJ\namSEJamREZakRkZYkhoZYUlqZIQlqZERlqRGRliSGhlhSWpkhCWpkRGWpEZGWJIaGWFJamSEJamR\nEZakRkZYkhoZYUlqZIQlqZERlqRGRliSGhlhSWpkhCWpkRGWpEZGWJIaGWFJamSEJamREZakRhNF\nOMkXk+xJ8kKSpSS3JHn/Qba7Osm+JC8l+WGSbdMbWZIWx6RXwucCXwfOAs4DjgJ+kOQdyxskuQq4\nArgUOBN4Edid5O1TmViSFklVrXkBTgAOAOeMrdsHXDn2+ljgZeCiQ3yN04FycXFxWcDl9MN1dL33\nhI8b7eg5gCQnA1uBO5c3qKoXgJ8BZ69zX5K0cNYc4SQBrgfurqqHRqu3MkR5acXmS6P3JEljNq3j\nz+4CTgU+PKVZJOmIs6Yr4STfAC4AtlfV02Nv7QcCbFnxR7aM3pMkjZk4wqMAfxL4WFU9Mf5eVe1l\niO2Ose2PZXia4p71jSpJi2ei2xFJdgEXAxcCLyZZvuJ9vqpeGf319cCXkjwKPA5cAzwJ3DqViSVp\ngUx6T/gyhl+8/XjF+r8Cvg1QVdcmOQb4JsPTEz8FPlFVv1/fqJK0eDJ6VrdvgOR04L7WISRpNs6o\nqvtX28DPjpCkRkZYkhoZYUlqZIQlqZERlqRGRliSGhlhSWpkhCWpkRGWpEZGWJIaGWFJamSEJamR\nEZakRkZYkhoZYUlqZIQlqZERlqRGRliSGhlhSWpkhCWpkRGWpEZGWJIaGWFJamSEJamREZakRkZY\nkhoZYUlqZIQlqZERlqRGRliSGhlhSWpkhCWpkRGWpEZGWJIaGWFJamSEJamREZakRkZYkhoZYUlq\nZIQlqZERlqRGRliSGhlhSWpkhCWpkRGWpEYTRTjJF5PsSfJCkqUktyR5/4ptbkhyYMVy23THlqTF\nMOmV8LnA14GzgPOAo4AfJHnHiu1uB7YAW0fLxeucU5IW0qZJNq6qC8ZfJ/ks8H/AGcDdY2/9rqqe\nWfd0krTg1ntP+DiggOdWrN8+ul3xcJJdSd61zv1I0kKa6Ep4XJIA1wN3V9VDY2/dDnwP2Au8F/gy\ncFuSs6uq1jOsJC2aNUcY2AWcCnx4fGVV3Tz28ldJfgk8BmwH7lrH/iRp4azpdkSSbwAXANur6unV\ntq2qvcCzwLa17EuSFtnEV8KjAH8S+GhVPfEmtj8ROB5YNdaSdCSa9DnhXcBngE8DLybZMlqOHr2/\nOcm1Sc5K8p4kO4B/Bx4Bdk97eEmad5PejrgMOBb4MbBvbLlo9P5rwAeAW4H/Af4Z+Dnwkap6dQrz\nStJCmfQ54VWjXVWvAB9f10SSdATxsyMkqZERlqRGRliSGhlhSWpkhCWpkRGWpEZGWJIaGWFJamSE\nJamREZakRkZYkhoZYUlqZIQlqZERlqRGRliSGhlhSWpkhCWpkRGWpEZGWJIaGWFJamSEJanRRojw\n0d0DSNKMHLZvGyHCJ3UPIEkzctLhNkhVvQVzrDJAcjxwPvA48ErrMJI0HUczBHh3Vf12tQ3bIyxJ\nR7KNcDtCko5YRliSGhlhSWpkhCWp0YaMcJLPJ9mb5OUk9yb50+6ZpiHJziQHViwPdc+1FknOTfL9\nJE+NjuPCg2xzdZJ9SV5K8sMk2zpmXYvDHV+SGw5yLm/rmvfNSvLFJHuSvJBkKcktSd5/kO3m8ty9\nmePbaOduw0U4yaeA64CdwAeBXwC7k5zQOtj0PAhsAbaOlnN6x1mzzcADwOXAGx6xSXIVcAVwKXAm\n8CLDeXz7WznkOqx6fCO38/pzefFbM9q6nAt8HTgLOA84CvhBkncsbzDn5+6wxzeycc5dVW2oBbgX\n+Mex1wGeBL7QPdsUjm0ncH/3HDM4rgPAhSvW7QOuHHt9LPAycFH3vFM6vhuAf+uebQrHdsLo+M5Z\n0HN3sOPbUOduQ10JJzkKOAO4c3ldDX/XfgSc3TXXlL1v9CPuY0m+k+Td3QNNW5KTGa4uxs/jC8DP\nWJzzCLB99CPvw0l2JXlX90BrcBzDlf5zsJDn7nXHN2bDnLsNFWGGf2u9DVhasX6J4Rtj3t0LfJbh\nvxC8DDgZ+EmSzZ1DzcBWhm/8RT2PMPw4ewnwF8AXgI8CtyVJ61QTGM16PXB3VS3/bmJhzt0hjg82\n2Lnb1LHTI1VV7R57+WCSPcCvgYsYfkTSnKiqm8de/irJL4HHgO3AXS1DTW4XcCrw4e5BZuSgx7fR\nzt1GuxJ+FniN4Yb5uC3A/rd+nNmqqueBR4C5+M3zBPYz3Ms/Is4jQFXtZfj+nYtzmeQbwAXA9qp6\neuythTh3qxzfG3Sfuw0V4ap6FbgP2LG8bvQjwg7gnq65ZiXJOxlO/KrfJPNm9E29n9efx2MZfmO9\ncOcRIMmJwPHMwbkcBeqTwMeq6onx9xbh3K12fIfYvvXcbcTbEV8DbkxyH7AHuBI4Brixc6hpSPJV\n4D8YbkH8CfAPwKvAdzvnWovRfextDFdNAKckOQ14rqp+w3Av7ktJHmX4hLxrGJ5yubVh3Imtdnyj\nZSfwPYZgbQO+wvBTze43frWNI8kuhsexLgReTLJ8xft8VS1/iuHcnrvDHd/ovG6sc9f9eMYhHiu5\nnOHkvwz8N/Ch7pmmdFzfZfhmfhl4ArgJOLl7rjUey0cZHv15bcXyL2Pb/D3D404vMXyDb+ueexrH\nx/AxhXcw/EP8CvC/wD8Bf9w995s4roMd02vAJSu2m8tzd7jj24jnzo+ylKRGG+qesCQdaYywJDUy\nwpLUyAhLUiMjLEmNjLAkNTLCktTICEtSIyMsSY2MsCQ1MsKS1MgIS1Kj/we8OL2mND55OQAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb1f96a090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the data\n",
    "rand_image = random.randint(0, 200)\n",
    "print (rand_image)\n",
    "plt.imshow(test_dataset[rand_image], cmap='gray', interpolation='nearest', vmin=0, vmax=255)\n",
    "print (test_labels[rand_image])\n",
    "plt.show()\n",
    "np.mean(test_dataset[rand_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the data\n",
    "pickle_file = 'train7d10class10000.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 79588342\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20495\n"
     ]
    }
   ],
   "source": [
    "#estimate the duplicates\n",
    "\n",
    "all_data = pickle.load(open('train7d10class10000.pickle', 'rb'))\n",
    "\n",
    "def count_duplicates(dataset1, dataset2):\n",
    "    hashes = [hashlib.sha1(x).hexdigest() for x in dataset1]\n",
    "    dup_indices = []\n",
    "    for i in range(0, len(dataset2)):\n",
    "        if hashlib.sha1(dataset2[i]).hexdigest() in hashes:\n",
    "            dup_indices.append(i)\n",
    "    return len(dup_indices)\n",
    "\n",
    "\n",
    "print(count_duplicates(all_data['test_dataset'], all_data['train_dataset']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# test a regression model\n",
    "\n",
    "train_dataset = all_data['train_dataset']\n",
    "train_labels = all_data['train_labels']\n",
    "test_dataset = all_data['test_dataset']\n",
    "test_labels = all_data['test_labels']\n",
    "\n",
    "print (len(train_dataset))\n",
    "print (len(train_labels))\n",
    "print (len(test_dataset))\n",
    "print (len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 trainsamples score: 0.326\n",
      "1000 trainsamples score: 0.456\n",
      "5000 trainsamples score: 0.524\n",
      "100000 trainsamples score: 0.815\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_score(train_dataset, train_labels, test_dataset, test_labels):\n",
    "    model = LogisticRegression()\n",
    "    train_flatten_dataset = np.array([x.flatten() for x in train_dataset])\n",
    "    test_flatten_dataset = np.array([x.flatten() for x in test_dataset])\n",
    "    model.fit(train_flatten_dataset, train_labels)\n",
    "\n",
    "    return model.score([x.flatten() for x in test_dataset], test_labels)\n",
    "\n",
    "print(\"100 trainsamples score: \" + str(get_score(train_dataset[:100], train_labels[:100], test_dataset, test_labels)))\n",
    "print(\"1000 trainsamples score: \" + str(get_score(train_dataset[:1000], train_labels[:1000], test_dataset, test_labels)))\n",
    "print(\"5000 trainsamples score: \" + str(get_score(train_dataset[:5000], train_labels[:5000], test_dataset, test_labels)))\n",
    "print(\"100000 trainsamples score: \" + str(get_score(train_dataset[:100000], train_labels[:100000], test_dataset, test_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
